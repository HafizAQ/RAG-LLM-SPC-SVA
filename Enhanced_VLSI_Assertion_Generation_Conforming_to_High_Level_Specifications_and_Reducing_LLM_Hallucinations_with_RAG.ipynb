{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0pAQWOxj6K6m"
      ],
      "authorship_tag": "ABX9TyMP/P/+dGXOtzOPNXgY6FSr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HafizAQ/RAG-LLM/blob/main/Enhanced_VLSI_Assertion_Generation_Conforming_to_High_Level_Specifications_and_Reducing_LLM_Hallucinations_with_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1) Reuired Libraries & API Configurations"
      ],
      "metadata": {
        "id": "1wdsQrwVtCv8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Y2ECh5lOsZAs"
      },
      "outputs": [],
      "source": [
        "#Anvil plateform for web-application (UI-support)\n",
        "!pip install anvil-uplink"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Connection with Anvil server\n",
        "import anvil.server\n",
        "anvil.server.connect(\"server_TXRVWQUIKXGA6DNIE5U2IHLR-OVSPMVWSVIISYPV2\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4lj5DdMCs95p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OpenAI library installation\n",
        "!pip install openai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WLTSr921uA9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Required libraries/ packages\n",
        "!pip install openai\n",
        "!pip install langchain_openai\n",
        "!pip install --upgrade --quiet langchain-text-splitters tiktoken #split by tokens tiktoken is created by openai for chunking\n",
        "!pip install langchain_chroma\n",
        "!pip install langchain"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Nj3RZvXNuDuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install TextLoader"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YGfoSQdYuIm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_experimental"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YDxWOZ9Curo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2)LLMs Agents"
      ],
      "metadata": {
        "id": "d3c5W0Etu9X-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### i) OpenAI: Chat Completion API: inputs commands (system, User) + output respons: llm_call_cc1(sys,usr)"
      ],
      "metadata": {
        "id": "gTffFkrhu_DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying Prompt Engineering via Chat Completion API using economical gpt-3.5-turbo model\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=\"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\")\n",
        "\n",
        "def llm_call_cc1(in_system_insts, in_user_req):\n",
        "  output_txt=\"\"\n",
        "  stream = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": in_system_insts},\n",
        "      {\"role\": \"user\", \"content\": in_user_req}\n",
        "    ],\n",
        "    stream=True,)\n",
        "  for part in stream:\n",
        "     ot=part.choices[0].delta.content or \"\"\n",
        "     output_txt=output_txt+ot\n",
        "  return output_txt"
      ],
      "metadata": {
        "id": "bbCs9ESiuxMj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####ii) Role: Specification Extracter from Chunk: chunk_spec(chunk)"
      ],
      "metadata": {
        "id": "SLXYRy9pvI7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instruction for Chat Completion API of GPT\n",
        "\n",
        "def chunk_spec(chunk):\n",
        "  in_system_insts = \"System: You are an expert in technical specifications within the hardware VLSI design flow. Approach each task as a distinct and independent request. Your task is to meticulously extract list of information about all the signals in one by one from the provided text chunk **'{chunk}'** in such output formate: \\n[signal name] \\t\\n [Signal description] \\t\\n[Signal Functionality] \\t\\n[Signal Property/constraints] \\t\\n[connection with other Signal] \\t\\n [additional information about signal]. \\n\\nThe signal information should provid aid in crafting detailed specification sentences. These specifications should be instrumental for a subsequent SystemVerilog Assertions (SVA) generator. Adhere to these guidelines: \\n 1.Extract information strictly from the text chunk '**'{chunk}'** provided. \\n 2.Refrain from introducing any additional information or assumptions beyond what is explicitly mentioned in chunk '**'{chunk}'**.\\n 3.In instances where chunk **'{chunk}'** lacks comprehensive details, extract useful keywords or phrases present in chunk **'{chunk}'** to assist in subsequent tasks(SVA generation).\"\n",
        "  # in_system_insts = \"System: You are an expert in technical specifications in hardware VLSI design flow. Treat each task as a new and independent request. Your task is to extract and list all relevant information (may include Signal & Variables, Conditions & Properties and expected behavior of the system) from the given text **'{chunk}'** that could help in defining detailed specification sentences and helpful in giving context to the SVA generator later. Follow these guidelines: 1. Make sure to only use the information provided in the text **'{chunk}'**. 2. Do not add any information or assumptions. 3. If the text **'{chunk}'** does not contain sufficient information, provide useful keywords from the text **'{chunk}'**.' User will provide the text chunk **'{chunk}'** for each request.\"\n",
        "  # in_system_insts = \"System: You are an accurate tool extract all specifications related informations from the given chunk **'chunk'** of text. The specification information must be relevant to the signals, it should be useful for specification engineer and specification analyst. The signal information should be helpful for systemVerilog assertions (SVA); \\n Specification could be one or more sentences which will help to a SystemVerilog Assertion property; \\n Do not provide any extra information, introduction or conclusion, there must be info related to SVA within the chunk **'chunk'** only in the response; \\n Please be specifi1c to the provided information.\"\n",
        "  # This includes identifying as [signal names], [their specific functionalities], and [any relevant conditions], [properties], and [expected behaviors] of\n",
        "\n",
        "  in_user_req = \"User: \\n Here is the text chunk:\\n\\n'{\"+chunk+\"}'\\n\\nPlease extract and list all relevant information for specifications from this text **'{chunk}'**\"\n",
        "  # in_user_req = \"User: \\n Follow the instructions in the System role always. \\n Keep those instructions in context all the time. \\n ... \\n\\n Please generate SystemVerilog assertion (SVA) against the given specification as:\\n **'chunk'**: \\n {\"+chunk+\"}\"\n",
        "  chunk_spec=llm_call_cc1(in_system_insts,in_user_req)\n",
        "  return chunk_spec"
      ],
      "metadata": {
        "id": "N66GY26-vIFC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####iii) Role: Signal Mapper: signal_map(hlsf,hdlImp)"
      ],
      "metadata": {
        "id": "HxcXQYQrvPF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instruction for Chat Completion API of GPT\n",
        "\n",
        "def signal_map(hlsf,hdlImp):\n",
        "  in_system_insts = \"You are a signal mapper with expertise in technical specifications and HDL (Hardware Description Language). Your task is to accept two inputs from the user each time: 1. A list of specifications related to information from a text chunk. \\n **'spec'**: \\n 2. HDL code provided in a text file **'hdl'**. You should replace the keywords in the specification (**'spec'**) with the relevant keywords from the HDL code, with similar meaning. The edited specifications should be useful for generating SystemVerilog Asertions (SVAs) in later stages. Follow these guidelines: \\n1. Treat each task as a new and independent request text chunk **'spec'**. \\n2. Only use the information provided in the text chunk **'spec'** and the HDL code **'hdl'**. \\n3. Do not add any information or make assumptions. \\n4. Do not change any part of the HDL code **'hdl'** \\n5. For global signals (global reset and global clk) of text chunk **'spec'** should be replace by globals signals (global reset and global clk) in the HDL **'hdl'**, place them separately in the following format:\\n   - global reset in HDL: [keyword for global reset signal in HDL]\\n   - global clock signal in HDL: [keyword for global clock signal in HDL]\"\n",
        "  # in_system_insts = \"System: You are a professional hardware verification engineer and specification analysit. You have experience in writing systemVerilog assertions from the specification text/ documents(usually given in natural language). \\n Please, every time follow the instructions given below: \\n 1. Write SystemVerilog Assertion against the given specification (**'spec'**) in natural language; \\n 2. **'spec'** could be one or more sentences which will refer to a single SystemVerilog Assertion property and please write only one SystemVerilog Assertion for that; \\n 3. Do not provide any extra information, introduction or conclusion, there must be systemverilog assertion only in the response; \\n 4. Most Importantly, user will provide you design retated text **'design'**, please use its signal notation into your generated systemverilog assertion which will be suitable related to **'spec'**; \\n 5. For your help, user will provide the contextual text (**'specf'**), you will analyz **'specf'** text for writing the SystemVerilog Assertion assertions from **'spec'**; \\n 6. Please struct to the syntax of systemverilog assertion language; \\n 7. Please be specific to the provided information.\"\n",
        "\n",
        "  in_user_req = \"Here is the list of specifications related information from the chunk \\n**'spec'**:\\n\\n{\"+hlsf+\"}\\n\\nAnd here is the HDL code \\n**'hdl'**:\\n\\n{\"+hdlImp+\"}\\n\\nPlease replace the keywords in the specification list with the relevant keywords from the HDL code and provide the edited specifications.\"\n",
        "  # in_user_req = \"User: \\n Follow the instructions in the System role always. \\n Keep those instructions in context all the time. \\n ... \\n\\n Please generate SystemVerilog assertion (SVA) against the given specification as:\\n **'spec'**: \\n {\"+hlsf+\"} \\n Please during assertion generation take symbols notations and other information as given in from design information text **'spec'**: in the follwoing \\n {\"+hdlImp+\"} \\n You can also take help from specification content as well \\n {\"+contentD+\"}\"\n",
        "  hlspecs=llm_call_cc1(in_system_insts,in_user_req)\n",
        "  return hlspecs"
      ],
      "metadata": {
        "id": "robiPCQzvOQN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####iv) Role: Grand Assertion Generator from Designer's side specification: designSpec_sva (contextDSF,spec)"
      ],
      "metadata": {
        "id": "mzNSiEfKvcn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instruction for Chat Completion API of GPT\n",
        "\n",
        "def designSpec_sva(contextDSF,spec):\n",
        "  in_system_insts = \"You are a expert system specializing in SystemVerilog Assertion (SVA) and hardware description languages (HDL). Your task is to generate SystemVerilog Assertions (SVA) based on provided specification sentence (**'spec'**) and ensure they are correctly formatted as formal concurrent SVA assertion with a simple implication template and runnable in a HDL file code. The formate should be consistant every time. You will receive two inputs from the user each time: \\n1. A specification sentence (**'spec'**) written in natural language. 2. Specification context text (**'spec_context'**) file. \\n Use the specification context text  (**'spec_context'**) to during the translation of an SVA  from the given specification (**'spec'**) is consistent with the specification context (**'spec_context'**). \\n The generated SVA must accurately represent the specification sentence (**'spec'**) and be correct and executable in HDL and should take global clock signal and global reset signals from specification context text (**'spec_context'**). \\n Follow these guidelines: \\n 1. Do not add any extra information. \\n 2. Do not include any natural language sentences while writing the SVA. \\n 3. Treat each request as a new and independent task, without maintaining any user history. \\n 4. after SVA generation against spec (**'spec'**), please provided relevant text from specification context text (**'spec_context'**) that showing conformance to the spec (**'spec'**) or contradiction to the given spec (**'spec'**) in following output formate: \\n [spec from (**'spec'**)] \\n [generated SVA against spec(**'spec'**)] \\n [Conforming or contradicting Context (**'spec_context'**) againt spec (**'spec'**)]\"\n",
        "  # in_system_insts = \"System: You are a professional hardware verification engineer and specification analysit. You have experience in writing systemVerilog assertions from the specification text/ documents(usually given in natural language). \\n Please, every time follow the instructions given below: \\n 1. Write SystemVerilog Assertion against the given specification (**'spec'**) in natural language; \\n 2. **'spec'** could be one or more sentences which will refer to a single SystemVerilog Assertion property and please write only one SystemVerilog Assertion for that; \\n 3. Do not provide any extra information, introduction or conclusion, there must be systemverilog assertion only in the response; \\n 4. Most Importantly, user will provide you design retated text **'design'**, please use its signal notation into your generated systemverilog assertion which will be suitable related to **'spec'**; \\n 5. For your help, user will provide the contextual text (**'specf'**), you will analyz **'specf'** text for writing the SystemVerilog Assertion assertions from **'spec'**; \\n 6. Please struct to the syntax of systemverilog assertion language; \\n 7. Please be specific to the provided information.\"\n",
        "\n",
        "  in_user_req = \"Here is the specification sentence\\n **'spec'**:\\n\\n {\"+spec+\"}\\n\\nAnd here is the specification context text **'spec_context'**:\\n\\n{\"+contextDSF+\"}\\n\\nPlease translate the specification sentence into a SystemVerilog Assertion (SVA).\"\n",
        "  # in_user_req = \"User: \\n Follow the instructions in the System role always. \\n Keep those instructions in context all the time. \\n ... \\n\\n Please generate SystemVerilog assertion (SVA) against the given specification as:\\n **'spec'**: \\n {\"+spec+\"} \\n Please during assertion generation take symbols notations and other information as given in from design information text **'spec'**: in the follwoing \\n {\"+contextDSF+\"} \\n You can also take help from specification content as well \\n {\"+contentD+\"}\"\n",
        "  assert_sva=llm_call_cc1(in_system_insts,in_user_req)\n",
        "  return assert_sva"
      ],
      "metadata": {
        "id": "JuUmpdAKvb-R"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Extra effort"
      ],
      "metadata": {
        "id": "gSS5O4Tgva3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a code to get text strings from Axi4L_HLSF_MD.md\n",
        "\n",
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader(\"Axi4L_HLSF_MD.md\")\n",
        "HLSF_MD_text = loader.load()\n",
        "print (f'You have {len(HLSF_MD_text)} document(s) in your data')\n",
        "print (f'There are {len(HLSF_MD_text[0].page_content)} characters in your document')\n",
        "print(HLSF_MD_text)\n",
        "print(type(HLSF_MD_text))\n",
        "\n",
        "HLSF_MDT_l1=chunk_spec(HLSF_MD_text[0].page_content)\n",
        "print(\"High Level Specification Document: \",HLSF_MDT_l1)\n",
        "\n",
        "#HDL design implementation file by Designer: hdlImpF_content\n",
        "hdlImpF_content=\"\"\n",
        "file = open(\"axi4_lite_HDLImpF.txt\", \"r\")\n",
        "hdlImpF_contentL=file.readlines()\n",
        "hdlImpF_content = ' '.join([str(elem) for i,elem in enumerate(hdlImpF_contentL)])\n",
        "file.close()\n",
        "\n",
        "HLSF_MDT_SigSpec_l=signal_map(HLSF_MDT_l1,hdlImpF_content)\n",
        "print(\"HLSF after HDL treatment: \",HLSF_MDT_SigSpec_l)\n",
        "\n",
        "#Design Specification to verify the design implementation: spf_list\n",
        "spf_contentL=\"\"\n",
        "file = open(\"Axi4L_Specs.txt\", \"r\")\n",
        "spf_contentL=file.readlines()\n",
        "file.close()\n",
        "\n",
        "for spc in spf_contentL:\n",
        "  print(\"Specification: \",spc)\n",
        "  print(\"SVA: \",designSpec_sva(HLSF_MDT_SigSpec_l,spc))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gfrxWA5ovuUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) A: RAG-Part"
      ],
      "metadata": {
        "id": "AxBwFjSWv_yl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####i) Preprocess the text file using NLP (NLTK) and Sklearn: preprocess_text(text)"
      ],
      "metadata": {
        "id": "XBnltsUGwGOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters but keep punctuations\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s.,;:!?\\'\\\"()-]', '', text)\n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def remove_semantic_repetition(sentences, threshold=0.7):\n",
        "    # Initialize TF-IDF Vectorizer\n",
        "    vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
        "    vectors = vectorizer.toarray()\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Identify and remove semantically similar sentences\n",
        "    unique_sentences = []\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        if all(similarity_matrix[i][j] < threshold for j in range(i)):\n",
        "            unique_sentences.append(sentence)\n",
        "\n",
        "    return unique_sentences\n",
        "\n",
        "def preprocess(input_file): #, output_file):\n",
        "    # with open(input_file, 'r', encoding='utf-8') as file:\n",
        "    #     text = file.read()\n",
        "\n",
        "    # Preprocess text\n",
        "    text = preprocess_text(input_file)\n",
        "\n",
        "    # Tokenize text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    #print(sentences)\n",
        "\n",
        "    # Remove semantic repetitions\n",
        "    unique_sentences = remove_semantic_repetition(sentences)\n",
        "\n",
        "    # Join unique sentences back to a single string\n",
        "    result_text = ' '.join(unique_sentences)\n",
        "\n",
        "    return result_text\n",
        "\n",
        "    # Write the result to a new file\n",
        "    # with open(output_file, 'w', encoding='utf-8') as file:\n",
        "    #     file.write(result_text)"
      ],
      "metadata": {
        "id": "nxArOjGbvzSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Langchain Platform Support"
      ],
      "metadata": {
        "id": "i4LNR6PKwL2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "os.environ['OPENAI_API_KEY'] = str(\"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\")\n",
        "#Splitter, embeddings, vector database\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter #Splitter\n",
        "from langchain.text_splitter import CharacterTextSplitter #Splitter\n",
        "from langchain_openai import OpenAIEmbeddings #Text embedding in vector form\n",
        "from langchain_chroma import Chroma #Vector database\n",
        "import re"
      ],
      "metadata": {
        "id": "rOW_N4CtwK5_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####ii) Chunker: Tiktoken with Recursive Character Splitter (Provided by OpenAi for Context Windows)"
      ],
      "metadata": {
        "id": "mwsCNaj0waxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Chunker\n",
        "def chunker(in_str_docF):\n",
        "  token_splitter_rcs= RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=0) #encoder merges tokens, means no of token could exetes the chunk size\n",
        "  t_rcs_chunks=token_splitter_rcs.split_documents(in_str_docF) #from specification documents(File-F)\n",
        "  return t_rcs_chunks"
      ],
      "metadata": {
        "id": "Qw3_Idx5wXiy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####(iii) Embedding: OpenAI embedding (Vector Embedding) & Vector Store: Chroma DB (Opne Source, interface with several technologies):\n",
        "#####chromaDBSF(t_rcs_chunks)"
      ],
      "metadata": {
        "id": "do1HFkkfwkZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ChromaD: saving in knowledge representation\n",
        "def chromaDBSF(t_rcs_chunks):\n",
        "  file_dbsF =Chroma.from_documents(t_rcs_chunks,OpenAIEmbeddings(), persist_directory=\"./chroma_dbF\")\n",
        "  return file_dbsF"
      ],
      "metadata": {
        "id": "vjtNdRkMwjwA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####iv) RAG_main: chromaDBSF_RP(hlsF_content,hdlImpF_content)"
      ],
      "metadata": {
        "id": "rn-ymrNSwi3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Inout High Level Soecification file, and Design file for synchromous\n",
        "def chromaDBSF_RP(hlsF_content,hdlImpF_content):\n",
        "  hlsF_content_p=preprocess_text(hlsF_content) #preprocessing test file 3(i)\n",
        "  doc_HLSF_content = Document(page_content=hlsF_content_p, metadata={\"User\": \"High Level Specs File document\"}) #Changing into document, so that split function could easily be applied\n",
        "  doc_HLSF_content_L=[doc_HLSF_content,] #list of document objects\n",
        "  hlsf_chunks=chunker(doc_HLSF_content_L) #3(ii) #return the chunks from the specification file\n",
        "  hlsf_chunks_specs=[]\n",
        "  print(\"hello\")\n",
        "  for hlsf_chunk in hlsf_chunks:\n",
        "    hlsf_chunk_spec=chunk_spec(hlsf_chunk.page_content) #2(ii)\n",
        "    hlsf_chunk_sig_spec=signal_map(hlsf_chunk_spec,hdlImpF_content) #2(iii)\n",
        "    # hlsf_chunk_sig_spec_c=hlsf_chunk_spec+\"\\n specs relevance: \"+hlsf_chunk_sig_spec\n",
        "    hlsf_chunk.page_content=hlsf_chunk_spec+\"\\n specs relevance: \"+hlsf_chunk_sig_spec\n",
        "    # hlsf_chunks_specs.append(hlsf_chunk_sig_spec_c)\n",
        "    hlsf_chunks_specs.append(hlsf_chunk)\n",
        "  file_dbsF=chromaDBSF(hlsf_chunks_specs) #3(iii)\n",
        "  return file_dbsF"
      ],
      "metadata": {
        "id": "yJ7pys1kwxp6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) B: LLM-Part"
      ],
      "metadata": {
        "id": "r7b7DBHzw2Kd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "######i) Specification as query to ChromaDB, it will return list of documents/ chunks (giving context) :spec_cdbSF(cdbSF,spcF)"
      ],
      "metadata": {
        "id": "yxeOS33Vw_z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spec_cdbSF(cdbSF,spcF):\n",
        "    q_embedding_vectorF = OpenAIEmbeddings().embed_query(spcF)# q_sim_search=cdb_doc.similarity_search(query) #q_sim_search\n",
        "    doc_chunksF = (cdbSF.similarity_search_by_vector(q_embedding_vectorF))#similarity_search_with_score(query)#similarity_search_by_vector_with_relevance_scores(query)\n",
        "    return doc_chunksF #returning documents against query"
      ],
      "metadata": {
        "id": "NYI6thniw0c5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####ii) Role: Grand Assertion Generator from Designer's side specification: designSpec_sva (contextDSF,spec)"
      ],
      "metadata": {
        "id": "6RjgdrQhxN81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instruction for Chat Completion API of GPT\n",
        "\n",
        "def designSpec_sva(contextDSF,spec):\n",
        "  in_system_insts = \"You are a SystemVerilog Assertion (SVA) expert. Your task is to translate a specification sentence (**'spec'**) written in natural language into a SystemVerilog Assertion. You will receive two inputs from the user each time: 1. A specification sentence (**'spec'**) written in natural language. 2. Specification context text (**'spec_context'**). Use the context to ensure the translated SVA is consistent with the specification context (**'spec_context'**). The generated SVA must accurately represent the specification sentence and be correct and executable. Follow these guidelines: 1. Do not add any extra information. 2. Do not include any natural language sentences other than the SVA. 3. Treat each request as a new and independent task, without maintaining any user history.\"\n",
        "  # in_system_insts = \"System: You are a professional hardware verification engineer and specification analysit. You have experience in writing systemVerilog assertions from the specification text/ documents(usually given in natural language). \\n Please, every time follow the instructions given below: \\n 1. Write SystemVerilog Assertion against the given specification (**'spec'**) in natural language; \\n 2. **'spec'** could be one or more sentences which will refer to a single SystemVerilog Assertion property and please write only one SystemVerilog Assertion for that; \\n 3. Do not provide any extra information, introduction or conclusion, there must be systemverilog assertion only in the response; \\n 4. Most Importantly, user will provide you design retated text **'design'**, please use its signal notation into your generated systemverilog assertion which will be suitable related to **'spec'**; \\n 5. For your help, user will provide the contextual text (**'specf'**), you will analyz **'specf'** text for writing the SystemVerilog Assertion assertions from **'spec'**; \\n 6. Please struct to the syntax of systemverilog assertion language; \\n 7. Please be specific to the provided information.\"\n",
        "\n",
        "  in_user_req = \"Here is the specification sentence\\n **'spec'**:\\n\\n {\"+spec+\"}\\n\\nAnd here is the specification context text **'spec_context'**:\\n\\n{\"+contextDSF+\"}\\n\\nPlease translate the specification sentence into a SystemVerilog Assertion (SVA).\"\n",
        "  # in_user_req = \"User: \\n Follow the instructions in the System role always. \\n Keep those instructions in context all the time. \\n ... \\n\\n Please generate SystemVerilog assertion (SVA) against the given specification as:\\n **'spec'**: \\n {\"+spec+\"} \\n Please during assertion generation take symbols notations and other information as given in from design information text **'spec'**: in the follwoing \\n {\"+contextDSF+\"} \\n You can also take help from specification content as well \\n {\"+contentD+\"}\"\n",
        "  assert_sva=llm_call_cc1(in_system_insts,in_user_req)\n",
        "  return assert_sva"
      ],
      "metadata": {
        "id": "Xvqd78YIxJlX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5) Assertion Generation on Collab"
      ],
      "metadata": {
        "id": "Iw3k9FV6xVIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####i) Propoeed Methodology: Combining RAG & Multi-LLM:  chromaDBSF_RP(hlsF, hdlImpF, spcFL)"
      ],
      "metadata": {
        "id": "4VF8pqdExWr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Main function\n",
        "import re\n",
        "from langchain_core.documents import Document #for creating document object from text file string to make it acceptable for ChromaDB\n",
        "\n",
        "def rag_llm(hlsF_content,hdlImpF_content,spf_contentL):\n",
        "  cdbSF=chromaDBSF_RP(hlsF_content,hdlImpF_content) #3(iv)#Passing dpcument as a list of document to chromaDBF function where, chunking, embedding and storing will be one after another\n",
        "  print(cdbSF)\n",
        "\n",
        "  assertionList=[]\n",
        "  for spec in spf_contentL:\n",
        "    DocsSpcHLS =spec_cdbSF(cdbSF,spec) #4(i)#will fetch design chunk releted to the specification\n",
        "    print(\"Docs SpecF conteent against spec sentence\",DocsSpcHLS)\n",
        "    assertsGen=designSpec_sva(DocsSpcHLS[0].page_content, spec)#4(ii)\n",
        "    assertionList.append(assertsGen)\n",
        "  return assertionList"
      ],
      "metadata": {
        "id": "CPcpKr5sxRJ-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####ii) Start function: reading file, or recieving specification file and returning SVA assertions"
      ],
      "metadata": {
        "id": "aitAdW_Mx4dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload all relevant files\n",
        "#Input files: HLSF, HDLImpF, DSpec\n",
        "#High Level Specification File by System Analyst: hlsF_content\n",
        "hlsF_content=\"\"\n",
        "file = open(\"Axi4L_HLSF.txt\", \"r\")\n",
        "hlsF_contentL=file.readlines()\n",
        "hlsF_content = ' '.join([str(elem) for i,elem in enumerate(hlsF_contentL)])\n",
        "file.close()\n",
        "\n",
        "#HDL design implementation file by Designer: hdlImpF_content\n",
        "hdlImpF_content=\"\"\n",
        "file = open(\"axi4_lite_HDLImpF.txt\", \"r\")\n",
        "hdlImpF_contentL=file.readlines()\n",
        "hdlImpF_content = ' '.join([str(elem) for i,elem in enumerate(hdlImpF_contentL)])\n",
        "file.close()\n",
        "\n",
        "#Design Specification to verify the design implementation: spf_list\n",
        "spf_contentL=\"\"\n",
        "file = open(\"Axi4L_Specs.txt\", \"r\")\n",
        "spf_contentL=file.readlines()\n",
        "file.close()\n",
        "\n",
        "assertionList = rag_llm(hlsF_content,hdlImpF_content,spf_contentL) #5(i)\n",
        "\n",
        "for item in assertionList:\n",
        "  print(item)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3Hqy76kAxtvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extra"
      ],
      "metadata": {
        "id": "0pAQWOxj6K6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Vector Store Function\n",
        "\n",
        "#####i) Preprocess the text file using NLP (NLTK) and Sklearn\n",
        "#####ii) Support framework: LangChain (Support Apps and Open Source)\n",
        "#####iii) Chunker: Tiktoken with Recursive Character Splitter (Provided by OpenAi for Context Windows)\n",
        "#####iv) Embedding: OpenAI embedding (Vector Embedding)  \n",
        "#####vi) Vector Store: Chroma DB (Opne Source, interface with several technologies)\n",
        "#####vii) Load vector Database against presistant directory"
      ],
      "metadata": {
        "id": "KbheToFLyaaf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zthjbl8Mx_oB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}