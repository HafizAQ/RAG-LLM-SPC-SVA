{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0pAQWOxj6K6m",
        "gSS5O4Tgva3x"
      ],
      "authorship_tag": "ABX9TyOy2v+g1q2Smkx7Ak9dHDNC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HafizAQ/RAG-LLM-SPC-SVA/blob/main/Enhanced_VLSI_Assertion_Generation_Conforming_to_High_Level_Specifications_and_Reducing_LLM_Hallucinations_with_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1) Reuired Libraries & API Configurations"
      ],
      "metadata": {
        "id": "1wdsQrwVtCv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Required libraries/ packages\n",
        "!pip install openai\n",
        "!pip install langchain_openai\n",
        "!pip install --upgrade --quiet langchain-text-splitters tiktoken #split by tokens tiktoken is created by openai for chunking\n",
        "!pip install langchain_chroma\n",
        "!pip install langchain\n",
        "!pip install TextLoader\n",
        "!pip install langchain_experimental\n",
        "!pip install -U langchain-chroma\n",
        "!pip install langchain_community"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Nj3RZvXNuDuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2)LLMs Agents"
      ],
      "metadata": {
        "id": "d3c5W0Etu9X-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2.(i) OpenAI: Chat Completion API: inputs commands (system, User) + output respons: llm_call_cc1(sys,usr)"
      ],
      "metadata": {
        "id": "gTffFkrhu_DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying Prompt Engineering via Chat Completion API using economical gpt-3.5-turbo model\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=\"sk-xxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
        "\n",
        "def llm_call_cc1(in_system_insts, in_user_req):\n",
        "  output_txt=\"\"\n",
        "  stream = client.chat.completions.create(\n",
        "    # model=\"gpt-4\",\n",
        "    model=\"gpt-3.5-turbo-0125\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": in_system_insts},\n",
        "      {\"role\": \"user\", \"content\": in_user_req}\n",
        "    ],\n",
        "    stream=True,\n",
        "    temperature=0.3)\n",
        "  for part in stream:\n",
        "     ot=part.choices[0].delta.content or \"\"\n",
        "     output_txt=output_txt+ot\n",
        "  return output_txt"
      ],
      "metadata": {
        "id": "bbCs9ESiuxMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2.(ii) Role: Specification Extracter from Chunk: chunk_spec(chunk)"
      ],
      "metadata": {
        "id": "SLXYRy9pvI7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instruction for Chat Completion API of GPT\n",
        "# Global Signals,\n",
        "def chunk_spec(chunk):\n",
        "  in_system_insts = \"You are an expert VLSI specification analyzer tasked with extracting signal information (Write address channels, Write data channels, Write response channels, Read address channels, Read data channels, Global/system signals) from the provided specification '{chunk}' to support SystemVerilog Assertion (SVA) generation. Each task is independent of the others. \\n\\n Task Instructions: \\n\\t 1. Objective: Extract signal details from **'{chunk}'** in the following structure: \\n\\t\\t [Signal Name]: Name of the signal. \\n\\t\\t [Signal Description]: Detailed description (e.g., type, width, purpose).\\n\\t\\t [Signal Functionality]: Explanation of the signal's role (e.g., data management, control flow). \\n\\t 2. Guidelines: \\n\\t\\t Only use information from **'{chunk}'**.\\n\\t\\t Do not make assumptions or add external information.\\n\\t\\t If details are missing, leave them as is without assumptions.\\n\\t 3. Purpose: Ensure the extracted signal information is accurate and clearly structured to assist in SystemVerilog Assertion (SVA) generation.\"\n",
        "\n",
        "  in_user_req = \"Here is the text **'{chunk}'**:\\n'{\"+chunk+\"}'\\n\\nPlease extract and list all relevant signal information for specifications from this text **'{chunk}'**, ensuring the output follows the format provided in the System Role prompt.\"\n",
        "\n",
        "  chunk_spec=llm_call_cc1(in_system_insts,in_user_req) #2.(i) GPT model Completion API\n",
        "  return chunk_spec"
      ],
      "metadata": {
        "id": "N66GY26-vIFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2.(iii) Role: Signal Mapper: signal_map(hlsf,hdlImp)"
      ],
      "metadata": {
        "id": "HxcXQYQrvPF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instruction for Chat Completion API of GPT\n",
        "def signal_map(hlsf,hdlImp):\n",
        "  in_system_insts = \"You are an intelligent signal mapping tool that links signal specifications to their definitions in HDL (SystemVerilog) code. Your task is to update the signal names in **'{spec}'** with their corresponding names in the HDL code **{'hdl'}** if corresponding signal names exit in **{'hdl'}**, ensuring accuracy for SystemVerilog Assertion (SVA) generation. Also extract both System Clock signals and System Reset Signal from **{'hdl'}** code and add into the updated **'{spec}'**. \\n\\n Task Overview: \\n\\t Input 1: Signal specifications from **'{spec}'**. \\n\\t Input 2: HDL code from **{'hdl'}**. \\n\\n Task Instructions: \\n\\t 1. Signal Name: Replace signal names in **'{spec}'** with their corresponding names from **{'hdl'}** (if available). If a signal is not found in **{'hdl'}**, leave it unchanged. \\n\\t 2. Signal Description & Functionality: Do not modify; retain as provided in **'{spec}'**. \\n\\t 3. System Signals (Mandatory): Always extract both System Reset and System Clock signals names from **{'hdl'}** code and add into the updated **'{spec}'**. \\n\\t 4. Signal Name Identification and adding to the updated **'{spec}'**: Find signal names from **'{spec}'** that correspond signal name of bit-vectors or buffers type logic in **{'hdl'}** and add into the updated **'{spec}'**. These are signals where the $isunknown() function is applicable (i.e., for detecting unknown values in bit-vector or buffer signals). \\n\\n Task Independence: \\n\\t Treat each task independently without modifying the HDL code **{'hdl'}**. \\n\\n Output: \\n\\t Final Output: Provide an updated version of **'{spec}'** that includes: \\n\\t Global signals (System Reset and System Clock) \\n\\n Signal name updates (where applicable from **{'hdl'}**),\\n\\t Bit-vector signals where $isunknown() applies.\\n\\n Ensure that the output is optimized for SVA generation with accurate signal mappings from **{'hdl'}**.\"\n",
        "\n",
        "\n",
        "  in_user_req = \"User Role: Here is the list of specifications-related information from the chunk \\n**'{spec}'**:\\n\\n{\"+hlsf+\"}\\n\\nAnd here is the HDL code \\n**{'hdl'}**:\\n\\n{\"+hdlImp+\"}\\n\\nPlease replace the keywords in the specification list with the relevant keywords/signal names from the HDL code and provide the edited specifications.\"\n",
        "\n",
        "  hlspecs=llm_call_cc1(in_system_insts,in_user_req) #2.(i) GPT model Completion API\n",
        "  return hlspecs"
      ],
      "metadata": {
        "id": "robiPCQzvOQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) A: RAG-Part"
      ],
      "metadata": {
        "id": "AxBwFjSWv_yl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3.(i) Preprocess the text file using NLP (NLTK) and Sklearn: preprocess_text(text)"
      ],
      "metadata": {
        "id": "XBnltsUGwGOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters but keep punctuations\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s.,;:!?\\'\\\"()-]', '', text)\n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def remove_semantic_repetition(sentences, threshold=0.7):\n",
        "    # Initialize TF-IDF Vectorizer\n",
        "    vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
        "    vectors = vectorizer.toarray()\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Identify and remove semantically similar sentences\n",
        "    unique_sentences = []\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        if all(similarity_matrix[i][j] < threshold for j in range(i)):\n",
        "            unique_sentences.append(sentence)\n",
        "\n",
        "    return unique_sentences\n",
        "\n",
        "def preprocess(input_file): #, output_file):\n",
        "    # with open(input_file, 'r', encoding='utf-8') as file:\n",
        "    #     text = file.read()\n",
        "\n",
        "    # Preprocess text\n",
        "    text = preprocess_text(input_file)\n",
        "\n",
        "    # Tokenize text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    #print(sentences)\n",
        "\n",
        "    # Remove semantic repetitions\n",
        "    unique_sentences = remove_semantic_repetition(sentences)\n",
        "\n",
        "    # Join unique sentences back to a single string\n",
        "    result_text = ' '.join(unique_sentences)\n",
        "\n",
        "    return result_text"
      ],
      "metadata": {
        "id": "nxArOjGbvzSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Langchain Platform Support"
      ],
      "metadata": {
        "id": "i4LNR6PKwL2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "os.environ['OPENAI_API_KEY'] = str(\"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
        "#Splitter, embeddings, vector database\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter #Splitter\n",
        "from langchain.text_splitter import CharacterTextSplitter #Splitter\n",
        "from langchain_openai import OpenAIEmbeddings #Text embedding in vector form\n",
        "from langchain_chroma import Chroma #Vector database\n",
        "import re"
      ],
      "metadata": {
        "id": "rOW_N4CtwK5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####3.(ii) Chunker: Tiktoken with Recursive Character Splitter (Provided by OpenAi for Context Windows)"
      ],
      "metadata": {
        "id": "mwsCNaj0waxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Chunker using tiktoken encoder to fit token count in GPT 3.5 input context window, applied as RecursiveCharacterTextSplitter\n",
        "def chunker(in_str_docF):\n",
        "  token_splitter_rcs= RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=2100, chunk_overlap=100) #encoder merges tokens, means no of tokens could exetes the chunk size #Recusively look the character and find count the token as per OpenAI\n",
        "  t_rcs_chunks=token_splitter_rcs.split_documents(in_str_docF) #from specification documents(File-F), as GPT 3.5 has 4096 token limt, we set 1500, token slitting and will contain the additiinal information tokens, for subsequent LLM model\n",
        "  return t_rcs_chunks"
      ],
      "metadata": {
        "id": "Qw3_Idx5wXiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####3.(iii) Embedding: OpenAI embedding (Vector Embedding) & Vector Store: Chroma DB (Opne Source, interface with several technologies):\n",
        "#####chromaDBSF(t_rcs_chunks)\n",
        "\n",
        "######file_dbsF.persist()"
      ],
      "metadata": {
        "id": "do1HFkkfwkZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ChromaD: saving in knowledge representation\n",
        "from langchain_community.vectorstores.chroma import Chroma\n",
        "def chromaDBSF(t_rcs_chunks):\n",
        "  file_dbsF =Chroma.from_documents(t_rcs_chunks,OpenAIEmbeddings(), persist_directory=\"./chroma_dbF\")\n",
        "  # file_dbsF.persist()\n",
        "  return file_dbsF"
      ],
      "metadata": {
        "id": "vjtNdRkMwjwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3.(iv) RAG_main: chromaDBSF_RP(hlsF_content,hdlImpF_content): return ChromaDBF"
      ],
      "metadata": {
        "id": "rn-ymrNSwi3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Input High Level Soecification file, and Design file for synchromous, output vectorDB\n",
        "def chromaDBSF_RP(hlsF_content,hdlImpF_content):\n",
        "  hlsF_content_p=preprocess_text(hlsF_content) #3(i) preprocessing test file\n",
        "  # print(\"HLSF after preprocessing: \",hlsF_content_p)\n",
        "  doc_HLSF_content = Document(page_content=hlsF_content_p, metadata={\"User\": \"High Level Specs File document\"}) #Changing into document, so that split function could easily be applied\n",
        "  doc_HLSF_content_L=[doc_HLSF_content,] #list of document objects\n",
        "  hlsf_chunks=chunker(doc_HLSF_content_L) #3(ii) #return the chunks from the specification file\n",
        "\n",
        "  hlsf_chunks_specs=[]\n",
        "  for hlsf_chunk in hlsf_chunks:\n",
        "    hlsf_chunk_spec=chunk_spec(hlsf_chunk.page_content) #2(ii) documents to page content as chunk: LLM-1: Chunk-Spec\n",
        "    hlsf_chunk_sig_spec=signal_map(hlsf_chunk_spec,hdlImpF_content) #2(iii) LLM-2: Chunk-Spec-Sig\n",
        "    hlsf_chunk.page_content=hlsf_chunk.page_content+ \"\\n Relevance Specs from the chunk:\\n \"+ hlsf_chunk_sig_spec  #Merge chunk with its relevant Specification\n",
        "\n",
        "    hlsf_chunks_specs.append(hlsf_chunk)\n",
        "\n",
        "  print(\"after loop on chunks\")\n",
        "  print(\"HLSF chunks specs list: \",hlsf_chunks_specs)\n",
        "  print(\"HLSF chunks specs type: \",type(hlsf_chunks_specs), \"length: \", len(hlsf_chunks_specs))\n",
        "  file_dbsF=chromaDBSF(hlsf_chunks_specs) #3(iii)  pass chunks as document to ChromaDB (Vector Database) with OpenAI encoding\n",
        "  # return file_dbsF\n",
        "  print(\"HLSF ChromaDB: \",type(file_dbsF))\n",
        "  return file_dbsF"
      ],
      "metadata": {
        "id": "yJ7pys1kwxp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) B: LLM-Part"
      ],
      "metadata": {
        "id": "r7b7DBHzw2Kd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "######4.(i) Specification as query to ChromaDB, it will return list of documents/ chunks (giving context) :spec_cdbSF(cdbSF,spcF)"
      ],
      "metadata": {
        "id": "yxeOS33Vw_z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spec_cdbSF(cdbSF,spcF):\n",
        "    q_embedding_vectorF = OpenAIEmbeddings().embed_query(spcF)# q_sim_search=cdb_doc.similarity_search(query) #q_sim_search\n",
        "    doc_chunksF = (cdbSF.similarity_search_by_vector(q_embedding_vectorF))#similarity_search_with_score(query)#similarity_search_by_vector_with_relevance_scores(query)\n",
        "    return doc_chunksF #returning documents against query"
      ],
      "metadata": {
        "id": "NYI6thniw0c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####4.(ii) Role: Grand Assertion Generator from Designer's side specification: designSpec_sva (contextDSF,spec)"
      ],
      "metadata": {
        "id": "6RjgdrQhxN81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LLM-3: Instruction for Chat Completion API of GPT\n",
        "\n",
        "def designSpec_sva(contextDSF,spec):\n",
        "  in_system_insts = \"You are a professional VLSI engineer skilled in SystemVerilog Assertions (SVA) and HDL. Your task is to generate SVA based on the provided specification sentence (**'spec'**) with correct formatting, simple implication structure, and readiness for HDL code execution.\\n\\n Task Instructions:\\n\\t Input 1: A specification sentence (**'spec'**) with signal names and their relationships. \\n\\t Input 2: A specification context text (**'spec_context'**) to align global signals (clock and reset) and other relevant details. \\n\\n Guidelines: \\n\\t i) Cross-reference signals in **'spec'** with **'spec_context'**. If signals are logic bit-vectors (e.g., address/data signals), apply $isunknown() for validity where needed. \\n\\t ii) Use **'spec_context'** to align signal names and global signals (clock and reset). \\n\\t iii) The generated SVA must follow a formal implication structure, ready for HDL use. \\n\\t iv) Avoid adding extra information or natural language explanations. \\n\\t v) Treat each task independently without user history. \\n\\n Output Format:\\n\\t [Specification from **'spec'**] \\n\\t [Generated SVA incorporating $isunknown() if applicable] \\n\\t [Context from **'spec_context'** confirming or contradicting **'spec'**] \\n\\n Additional Requirement: \\n\\t Ensure global clock and reset signals are derived from **'spec_context'**.\"\n",
        "\n",
        "  in_user_req = \"Here is the specification sentence\\n **'spec'**:\\n\\n {\"+spec+\"}\\n\\nAnd here is the specification context text **'spec_context'**:\\n\\n{\"+contextDSF+\"}\\n\\nPlease translate the specification sentence (**'spec'**) into a SystemVerilog Assertion (SVA).\"\n",
        "\n",
        "  assert_sva=llm_call_cc1(in_system_insts,in_user_req)\n",
        "  return assert_sva"
      ],
      "metadata": {
        "id": "JuUmpdAKvb-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5) Assertion Generation on Collab"
      ],
      "metadata": {
        "id": "Iw3k9FV6xVIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####5.(i) Propoeed Methodology: Combining RAG & Multi-LLM:  chromaDBSF_RP(hlsF, hdlImpF, spcFL)"
      ],
      "metadata": {
        "id": "4VF8pqdExWr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Main function\n",
        "import re\n",
        "from langchain_core.documents import Document #for creating document object from text file string to make it acceptable for ChromaDB\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "def rag_llm(hlsF_content,hdlImpF_content,spf_contentL):\n",
        "\n",
        "  #####Either: buildining knowledge base/ chroma DB from scratch\n",
        "\n",
        "  #cdbSF=chromaDBSF_RP(hlsF_content,hdlImpF_content) #3(iv)#Passing dpcument as a list of document to chromaDB function tion where, chunking, embedding and storing will be one after another\n",
        "\n",
        "  #####OR: Use exiting knowledge-base/ Chroma DB\n",
        "\n",
        "  cdbSF = Chroma(persist_directory=\"./chroma_dbF\", embedding_function=OpenAIEmbeddings())\n",
        "  print(\"Presistant ChromaDB .get(): \", cdbSF.get())\n",
        "\n",
        "  assertionList=[]\n",
        "  for spec in spf_contentL:\n",
        "    DocsSpcHLS =spec_cdbSF(cdbSF,spec) #4(i)#will fetch suitable chunks/documents from ChromaDB, quering as specification\n",
        "    print(\"Docs SpecF conteent against spec sentence\",DocsSpcHLS)\n",
        "    assertsGen=designSpec_sva(DocsSpcHLS[0].page_content, spec)#4(ii) passing chunk and specs and getting assertions in SVA\n",
        "    assertionList.append(assertsGen)\n",
        "  return assertionList"
      ],
      "metadata": {
        "id": "CPcpKr5sxRJ-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####5.(ii) Start function: reading file, or recieving specification file and returning SVA assertions"
      ],
      "metadata": {
        "id": "aitAdW_Mx4dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload all relevant files\n",
        "#Input files: HLSF, HDLImpF, DSpec\n",
        "#High Level Specification File by System Analyst: hlsF_content\n",
        "hlsF_content=\"\"\n",
        "# file = open(\"Axi4L_HLSF.txt\", \"r\")\n",
        "fileS = open(\"Axi4L_HLSF_MD.md\", \"r\")\n",
        "hlsF_contentL=fileS.readlines()\n",
        "hlsF_content = ' '.join([str(elem) for i,elem in enumerate(hlsF_contentL)])\n",
        "fileS.close()\n",
        "\n",
        "#HDL design implementation file by Designer: hdlImpF_content\n",
        "hdlImpF_content=\"\"\n",
        "fileD = open(\"axi4_lite_if.sv\", \"r\") #HDL file for text synchronization\n",
        "hdlImpF_contentL=fileD.readlines()\n",
        "hdlImpF_content = ' '.join([str(elem) for i,elem in enumerate(hdlImpF_contentL)])\n",
        "fileD.close()\n",
        "\n",
        "#Design Specification to verify the design implementation: spf_list\n",
        "spf_contentL=\"\"\n",
        "fileSpcD = open(\"Axi4L_Specs.txt\", \"r\")\n",
        "spf_contentL=fileSpcD.readlines()\n",
        "fileSpcD.close()\n",
        "\n",
        "# rag_llm(hlsF_content,hdlImpF_content,spf_contentL) #5(i)\n",
        "assertionList = rag_llm(hlsF_content,hdlImpF_content,spf_contentL) #5(i)\n",
        "\n",
        "for item in assertionList:\n",
        "  print(item)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3Hqy76kAxtvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extra"
      ],
      "metadata": {
        "id": "0pAQWOxj6K6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Vector Store Function\n",
        "\n",
        "#####i) Preprocess the text file using NLP (NLTK) and Sklearn\n",
        "#####ii) Support framework: LangChain (Support Apps and Open Source)\n",
        "#####iii) Chunker: Tiktoken with Recursive Character Splitter (Provided by OpenAi for Context Windows)\n",
        "#####iv) Embedding: OpenAI embedding (Vector Embedding)  \n",
        "#####vi) Vector Store: Chroma DB (Opne Source, interface with several technologies)\n",
        "#####vii) Load vector Database against presistant directory"
      ],
      "metadata": {
        "id": "KbheToFLyaaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Download Zip file"
      ],
      "metadata": {
        "id": "kOdahyq21-MA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/file.zip /content/chroma_dbF2"
      ],
      "metadata": {
        "id": "QZZSjZJ42ENX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "UUmf0oHV2FCc",
        "outputId": "60ae9bef-47c9-461a-dd93-83a6b41ec8ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f4306f32-7826-4650-87cb-f4672bc3c7cd\", \"file.zip\", 135163)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####PDF to MarkDown converison"
      ],
      "metadata": {
        "id": "IGlrRwFvbvSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade pymupdf"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mNfUKhxzddI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymupdf\n",
        "from pymupdf_rag import to_markdown  # import Markdown converter\n",
        "\n",
        "doc = pymupdf.open(\"IHI0022E_axi4_lite.pdf\")  # open input PDF\n",
        "\n",
        "# define desired pages: this corresponds “-pages 1-10,15,20-N”\n",
        "page_list = list(range(9)) + [14] + list(range(19, len(doc)-1))\n",
        "\n",
        "# get markdown string for all pages\n",
        "md_text = to_markdown(doc, pages=page_list)\n",
        "\n",
        "# write markdown string to some file\n",
        "output = open(\"IHI0022E_axi4_lite.md\", \"w\")\n",
        "output.write(md_text)\n",
        "output.close()"
      ],
      "metadata": {
        "id": "KZCefS2j2PN0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}